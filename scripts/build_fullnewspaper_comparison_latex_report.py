#!/usr/bin/env python3
"""
Build a LaTeX writeup for sampled-vs-full-newspaper longitudinal rerun.

This report is purpose-built for the "same 20 cities, all available issues"
fork and uses artifacts from:
  - <fork-run>/longitudinal
  - <fork-run>/longitudinal/comparison_vs_base
"""

from __future__ import annotations

import argparse
import datetime as dt
import hashlib
import json
import shutil
import subprocess
from pathlib import Path
from typing import Any

import pandas as pd


def _read_csv(path: Path) -> pd.DataFrame:
    if not path.is_file():
        return pd.DataFrame()
    try:
        return pd.read_csv(path)
    except Exception:
        return pd.DataFrame()


def _read_json(path: Path) -> dict[str, Any]:
    if not path.is_file():
        return {}
    try:
        return json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        return {}


def _tex_escape(s: Any) -> str:
    return (
        str(s)
        .replace("\\", "\\textbackslash{}")
        .replace("&", "\\&")
        .replace("%", "\\%")
        .replace("$", "\\$")
        .replace("#", "\\#")
        .replace("_", "\\_")
        .replace("{", "\\{")
        .replace("}", "\\}")
        .replace("~", "\\textasciitilde{}")
        .replace("^", "\\textasciicircum{}")
    )


def _write_table_tex(path: Path, df: pd.DataFrame, cols: list[str], float_cols: list[str] | None = None) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    if df.empty:
        path.write_text("\\begin{tabular}{l}\n\\toprule\nNo data\\\\\n\\bottomrule\n\\end{tabular}\n", encoding="utf-8")
        return

    keep = [c for c in cols if c in df.columns]
    if not keep:
        keep = list(df.columns[: min(8, len(df.columns))])

    sub = df[keep].copy()
    float_cols = float_cols or []
    for c in sub.columns:
        if c in float_cols or pd.api.types.is_float_dtype(sub[c]):
            sub[c] = pd.to_numeric(sub[c], errors="coerce").map(lambda v: "" if pd.isna(v) else f"{float(v):.3f}")

    lines: list[str] = []
    lines.append("\\begin{tabular}{" + "l" * len(sub.columns) + "}")
    lines.append("\\toprule")
    lines.append(" & ".join(_tex_escape(c) for c in sub.columns) + " \\\\")
    lines.append("\\midrule")
    for row in sub.itertuples(index=False):
        lines.append(" & ".join(_tex_escape(v) for v in row) + " \\\\")
    lines.append("\\bottomrule")
    lines.append("\\end{tabular}")
    path.write_text("\n".join(lines) + "\n", encoding="utf-8")


def _parse_args() -> argparse.Namespace:
    ap = argparse.ArgumentParser(description="Build fork rerun comparison LaTeX report.")
    ap.add_argument(
        "--base-run-dir",
        default="/Users/saulrichardson/projects/newspapers/newspaper-analysis/reports/runs/pi_v1_run60",
        help="Baseline run directory.",
    )
    ap.add_argument(
        "--fork-run-dir",
        default="/Users/saulrichardson/projects/newspapers/newspaper-analysis/reports/runs/pi_v1_run60_fullnewspaper",
        help="Full-newspaper fork run directory.",
    )
    ap.add_argument(
        "--output-dir",
        default="",
        help="Output report directory (default: <fork-run>/reports/fullnewspaper_comparison).",
    )
    ap.add_argument(
        "--title",
        default="Full-Newspaper Fork Rerun: What Changed vs Sampled Panels",
        help="Report title.",
    )
    ap.add_argument("--compile-pdf", action=argparse.BooleanOptionalAction, default=True)
    return ap.parse_args()


def main() -> None:
    args = _parse_args()
    base_run = Path(args.base_run_dir).expanduser().resolve()
    fork_run = Path(args.fork_run_dir).expanduser().resolve()
    lon_dir = fork_run / "longitudinal"
    cmp_dir = lon_dir / "comparison_vs_base"
    out_dir = Path(args.output_dir).expanduser().resolve() if str(args.output_dir).strip() else (fork_run / "reports" / "fullnewspaper_comparison")
    figs = out_dir / "figures"
    tbls = out_dir / "tables"
    out_dir.mkdir(parents=True, exist_ok=True)
    figs.mkdir(parents=True, exist_ok=True)
    tbls.mkdir(parents=True, exist_ok=True)

    overall = _read_json(cmp_dir / "comparison_overall_metrics.json")
    change_counts = _read_csv(cmp_dir / "comparison_change_counts.csv")
    city = _read_csv(cmp_dir / "comparison_city_summary.csv")
    dist = _read_csv(cmp_dir / "comparison_city_distribution_shift.csv")
    cat_delta = _read_csv(cmp_dir / "comparison_category_delta_summary.csv")
    summary = _read_csv(lon_dir / "city_longitudinal_summary.csv")
    first_events = _read_csv(lon_dir / "city_first_zoning_events.csv")
    selected = _read_csv(fork_run / "panels" / "selected_panel_issues.csv")

    # Copy figures generated by compare_longitudinal_runs.py
    src_figs = [
        ("issue_expansion_top15.png", "issue_expansion_top15.png"),
        ("core_output_change_counts.png", "core_output_change_counts.png"),
        ("late_stage_distribution_shift_top15.png", "late_stage_distribution_shift_top15.png"),
    ]
    for src_name, dst_name in src_figs:
        src = cmp_dir / "figures" / src_name
        dst = figs / dst_name
        if src.is_file():
            shutil.copy2(src, dst)

    # Build report tables.
    top_expand = pd.DataFrame()
    if not city.empty:
        top_expand = city[
            ["city_key", "selected_issue_count_base", "selected_issue_count_fork", "issue_count_delta", "panel_chars_delta"]
        ].copy()
        top_expand = top_expand.sort_values("issue_count_delta", ascending=False).head(15)

    status_changes = pd.DataFrame()
    if not city.empty:
        keep = [
            "city_key",
            "first_full_status_base",
            "first_full_status_fork",
            "first_full_issue_id_base",
            "first_full_issue_id_fork",
            "first_full_date_delta_days",
        ]
        status_changes = city[city["first_full_status_base"].astype(str) != city["first_full_status_fork"].astype(str)][keep].copy()
        status_changes = status_changes.sort_values(["first_full_status_base", "city_key"])

    signal_changes = pd.DataFrame()
    if not city.empty:
        keep = [
            "city_key",
            "first_zoning_signal_type_base",
            "first_zoning_signal_type_fork",
            "first_zoning_issue_id_base",
            "first_zoning_issue_id_fork",
            "first_zoning_date_delta_days",
        ]
        signal_changes = city[city["first_zoning_signal_type_base"].astype(str) != city["first_zoning_signal_type_fork"].astype(str)][keep].copy()
        signal_changes = signal_changes.sort_values("city_key")

    top_dist = pd.DataFrame()
    if not dist.empty:
        top_dist = dist[
            ["city_key", "city_name", "state_abbr", "early_js_divergence", "mid_js_divergence", "late_js_divergence"]
        ].copy()
        top_dist = top_dist.sort_values("late_js_divergence", ascending=False).head(12)

    cat_delta_top = pd.DataFrame()
    if not cat_delta.empty:
        cat_delta_top = cat_delta.sort_values(["stage", "mean_abs_delta"], ascending=[True, False]).groupby("stage", as_index=False).head(6)

    # Write tex tables.
    _write_table_tex(tbls / "change_counts.tex", change_counts, ["metric", "changed_city_count", "changed_city_share"], float_cols=["changed_city_share"])
    _write_table_tex(
        tbls / "top_issue_expansion.tex",
        top_expand,
        ["city_key", "selected_issue_count_base", "selected_issue_count_fork", "issue_count_delta", "panel_chars_delta"],
        float_cols=["panel_chars_delta"],
    )
    _write_table_tex(
        tbls / "first_full_status_changes.tex",
        status_changes,
        ["city_key", "first_full_status_base", "first_full_status_fork", "first_full_issue_id_base", "first_full_issue_id_fork", "first_full_date_delta_days"],
        float_cols=["first_full_date_delta_days"],
    )
    _write_table_tex(
        tbls / "first_signal_type_changes.tex",
        signal_changes,
        ["city_key", "first_zoning_signal_type_base", "first_zoning_signal_type_fork", "first_zoning_issue_id_base", "first_zoning_issue_id_fork", "first_zoning_date_delta_days"],
        float_cols=["first_zoning_date_delta_days"],
    )
    _write_table_tex(
        tbls / "top_distribution_shift.tex",
        top_dist,
        ["city_key", "city_name", "state_abbr", "early_js_divergence", "mid_js_divergence", "late_js_divergence"],
        float_cols=["early_js_divergence", "mid_js_divergence", "late_js_divergence"],
    )
    _write_table_tex(
        tbls / "category_delta_summary.tex",
        cat_delta_top,
        ["stage", "category", "mean_delta", "mean_abs_delta"],
        float_cols=["mean_delta", "mean_abs_delta"],
    )

    # Run integrity values.
    n_city = int(summary["city_key"].nunique()) if not summary.empty else 0
    parse_share = float(pd.to_numeric(summary.get("parse_valid"), errors="coerce").mean()) if not summary.empty else float("nan")
    schema_share = float(pd.to_numeric(summary.get("schema_valid"), errors="coerce").mean()) if not summary.empty else float("nan")
    overflow_count = int(pd.to_numeric(summary.get("overflow_applied"), errors="coerce").fillna(0).astype(int).sum()) if not summary.empty else 0
    clipped_issue_total = int(pd.to_numeric(summary.get("clipped_issue_count"), errors="coerce").fillna(0).sum()) if not summary.empty else 0
    total_issues_fork = int(len(selected)) if not selected.empty else 0
    total_issues_base = int(_read_csv(base_run / "panels" / "selected_panel_issues.csv").shape[0])

    # Write tex.
    report_date = dt.datetime.now().strftime("%Y-%m-%d")
    doc: list[str] = []
    doc.append("\\documentclass[11pt]{article}")
    doc.append("\\usepackage[margin=1in]{geometry}")
    doc.append("\\usepackage{graphicx}")
    doc.append("\\usepackage{booktabs}")
    doc.append("\\usepackage{float}")
    doc.append("\\usepackage{hyperref}")
    doc.append("\\title{" + _tex_escape(args.title) + "}")
    doc.append("\\author{Automated fork rerun pipeline}")
    doc.append("\\date{" + _tex_escape(report_date) + "}")
    doc.append("\\begin{document}")
    doc.append("\\maketitle")
    doc.append("\\tableofcontents")
    doc.append("\\clearpage")

    doc.append("\\section{High-level approach}")
    doc.append(
        "This rerun keeps the same 20-city panel set but changes one core input decision: instead of a sampled panel "
        "(roughly 8--10 issues per city), each city request includes all locally available issue transcripts in the forked run. "
        "The longitudinal extraction is still one LLM call per city panel, with issues ordered chronologically."
    )
    doc.append("\\begin{itemize}")
    doc.append(f"\\item Cities analyzed: {n_city}")
    doc.append(f"\\item Sampled-panel issue count (prior run): {total_issues_base}")
    doc.append(f"\\item Full-newspaper issue count (fork run): {total_issues_fork}")
    if overall:
        doc.append(
            f"\\item Mean issues per city: {overall.get('issue_count_mean_base', float('nan')):.2f} "
            f"$\\rightarrow$ {overall.get('issue_count_mean_fork', float('nan')):.2f}"
        )
    if overflow_count == 0:
        doc.append("\\item Context overflow clipping: none")
    else:
        doc.append(
            f"\\item Context overflow clipping: {overflow_count} city panels (total clipped issues = {clipped_issue_total})"
        )
    doc.append(f"\\item Parse-valid share (fork run): {parse_share:.3f}" if pd.notna(parse_share) else "\\item Parse-valid share (fork run): NA")
    doc.append(f"\\item Schema-valid share (fork run): {schema_share:.3f}" if pd.notna(schema_share) else "\\item Schema-valid share (fork run): NA")
    doc.append("\\end{itemize}")

    doc.append("\\section{What changed after full-newspaper rerun}")
    doc.append(
        "This section compares outputs from the sampled run to the full-newspaper fork run for the same city set."
    )
    doc.append("\\begin{table}[H]\\centering\\caption{Core output change counts across cities}\\input{tables/change_counts.tex}\\end{table}")
    if (figs / "core_output_change_counts.png").is_file():
        doc.append(
            "\\begin{figure}[H]\\centering\\includegraphics[width=0.82\\textwidth]{figures/core_output_change_counts.png}"
            "\\caption{Number of cities with changed core outputs after full-newspaper rerun.}\\end{figure}"
        )

    doc.append("\\section{Where coverage expansion was largest}")
    doc.append("\\begin{table}[H]\\centering\\caption{Largest issue-count expansions by city}\\input{tables/top_issue_expansion.tex}\\end{table}")
    if (figs / "issue_expansion_top15.png").is_file():
        doc.append(
            "\\begin{figure}[H]\\centering\\includegraphics[width=0.86\\textwidth]{figures/issue_expansion_top15.png}"
            "\\caption{Top cities by additional issues included in full-newspaper run.}\\end{figure}"
        )

    doc.append("\\section{First-zoning and first-full detection differences}")
    doc.append(
        "Most movement occurred in first-full-ordinance detection status and anchor issue IDs, indicating that fuller city history "
        "often surfaces ordinance text not present in short sampled panels."
    )
    doc.append("\\begin{table}[H]\\centering\\caption{Cities with changed first-full status}\\input{tables/first_full_status_changes.tex}\\end{table}")
    doc.append("\\begin{table}[H]\\centering\\caption{Cities with changed first-signal type}\\input{tables/first_signal_type_changes.tex}\\end{table}")

    doc.append("\\section{Content-composition shifts}")
    doc.append(
        "We compare sampled vs full runs using JS divergence on category distributions by stage (early/mid/late). "
        "Larger values imply larger composition changes under full coverage."
    )
    doc.append("\\begin{table}[H]\\centering\\caption{Largest stage-composition shift cities (by late-stage JS)}\\input{tables/top_distribution_shift.tex}\\end{table}")
    doc.append("\\begin{table}[H]\\centering\\caption{Category delta summary (largest absolute movers)}\\input{tables/category_delta_summary.tex}\\end{table}")
    if (figs / "late_stage_distribution_shift_top15.png").is_file():
        doc.append(
            "\\begin{figure}[H]\\centering\\includegraphics[width=0.86\\textwidth]{figures/late_stage_distribution_shift_top15.png}"
            "\\caption{Cities with largest late-stage category distribution shifts, sampled vs full rerun.}\\end{figure}"
        )

    doc.append("\\section{Bottom line}")
    if overall:
        c1 = int(overall.get("changed_first_signal_type_city_count", 0))
        c2 = int(overall.get("changed_first_full_status_city_count", 0))
        c3 = int(overall.get("changed_dimensional_specificity_trend_city_count", 0))
        doc.append(
            f"With the same 20-city set, moving to full-newspaper panels changed first-signal type in {c1} cities, "
            f"first-full status in {c2} cities, and dimensional-specificity trend in {c3} cities."
        )
    doc.append(
        "The main practical effect is improved ordinance-detection coverage and richer phase structure in several cities, "
        "while broad complexity-direction results remain relatively stable."
    )

    doc.append("\\section{Reproducibility}")
    doc.append(f"Base run: \\path{{{_tex_escape(str(base_run))}}}.")
    doc.append(f"Fork run: \\path{{{_tex_escape(str(fork_run))}}}.")
    doc.append(f"Comparison artifacts: \\path{{{_tex_escape(str(cmp_dir))}}}.")
    doc.append("\\end{document}")

    tex_path = out_dir / "report.tex"
    tex_path.write_text("\n".join(doc) + "\n", encoding="utf-8")

    makefile = [
        "LATEXMK ?= latexmk",
        "",
        "all: report.pdf",
        "",
        "report.pdf: report.tex",
        "\t$(LATEXMK) -pdf -interaction=nonstopmode -halt-on-error report.tex",
        "",
        "clean:",
        "\t$(LATEXMK) -C",
        "",
        ".PHONY: all clean",
    ]
    (out_dir / "Makefile").write_text("\n".join(makefile) + "\n", encoding="utf-8")

    compiled_pdf = False
    if bool(args.compile_pdf):
        latexmk = shutil.which("latexmk")
        if latexmk:
            try:
                subprocess.run(
                    [latexmk, "-pdf", "-interaction=nonstopmode", "-halt-on-error", "report.tex"],
                    cwd=str(out_dir),
                    check=True,
                )
                compiled_pdf = True
            except Exception:
                compiled_pdf = False

    prov = {
        "created_at": dt.datetime.now().isoformat(timespec="seconds"),
        "script_sha256": hashlib.sha256(Path(__file__).read_bytes()).hexdigest(),
        "base_run_dir": str(base_run),
        "fork_run_dir": str(fork_run),
        "comparison_dir": str(cmp_dir),
        "output_dir": str(out_dir),
        "compiled_pdf": bool(compiled_pdf),
        "n_city": n_city,
        "total_issues_base": total_issues_base,
        "total_issues_fork": total_issues_fork,
        "overflow_count": overflow_count,
    }
    (out_dir / "provenance.json").write_text(json.dumps(prov, indent=2, sort_keys=True) + "\n", encoding="utf-8")

    print(
        "Done. "
        f"report_tex={tex_path} "
        f"report_pdf={(out_dir / 'report.pdf')} "
        f"compiled_pdf={compiled_pdf}"
    )


if __name__ == "__main__":
    main()
